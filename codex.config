model = "gpt-5.1-codex-max"
model_reasoning_effort = "xhigh"
model_provider = "amd-openai"
# mcp mode
approval_policy = "never"
sandbox_mode    = "danger-full-access"

[model_providers.amd-openai]
name = "AMD LLM Gateway (OpenAI-compatible)"
# NOTE the capital O/N in OpenAI — match your gateway exactly.
base_url = "https://llm-api.amd.com/OpenAI"

# Codex wire protocol to use. AMD’s /OpenAI/responses expects "responses".
# If your route is /OpenAI/chat/completions, switch this to "chat".
wire_api = "responses"

# If your gateway needs an API version (many Azure-style proxies do), uncomment:
query_params = { api-version = "2025-04-01-preview" }

# Pull the AMD subscription key from an env var and send it as a header:
env_http_headers = { "Ocp-Apim-Subscription-Key" = "LLM_GATEWAY_KEY" }

request_max_retries = 4
stream_idle_timeout_ms = 300000